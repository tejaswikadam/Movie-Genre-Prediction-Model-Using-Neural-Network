{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPreprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MGSebQgpDzA",
        "outputId": "4590897b-2384-45ba-ebcb-56a82edbf6ec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gETNwoRJlox"
      },
      "source": [
        "# import libraries \n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle as pkl\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import re\n",
        "import json\n",
        "import csv\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 300)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten, Lambda\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers, regularizers, optimizers\n",
        "from keras.callbacks import History, CSVLogger\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score, auc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1rRBzEpt09R"
      },
      "source": [
        "# Preprocessing the  data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIRLA1Mrpgde",
        "outputId": "977bdb2d-a79e-4336-ba9a-d035ac92bd81"
      },
      "source": [
        "#Lemmetizing Function (Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, \n",
        "#normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def nltk2wn_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return wordnet.NOUN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkcbWQa5uP6I"
      },
      "source": [
        "#Tokenize w/lemmetization AFTER removing stopwords \n",
        "#https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
        "def tokenize(plot, stop_words, lemmatize = False):\n",
        "    \n",
        "    def re_sub(pattern, replace):\n",
        "        return re.sub(pattern, replace, plot)\n",
        "    \n",
        "    plot = plot.lower() #lowercase\n",
        "    plot = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,/.\\d]*\", \"DG\") #generic tag for numbers\n",
        "    plot = re_sub(r\"([!?.]){2,}\", r\"\\1\") #Convert multiple punctuations to the last punctuation mark\n",
        "    plot = plot.replace('-',' ') #separating hyphenated words\n",
        "    plot = plot.replace('_','') #remove underscores\n",
        "    plot = re_sub(r'(?<!\\w)([a-zA-Z])\\.', r'\\1') #remove periods from abbreviations\n",
        "    plot = re_sub('[^\\w\\s\\.\\?\\!\\']','') #remove punctuation besides sentence completers and apostrophes\n",
        "    sentences = nltk.sent_tokenize(plot)\n",
        "    words = list(map(nltk.word_tokenize, sentences))\n",
        "    words = [[x for x in w if not x in stop_words] for w in words]\n",
        "\n",
        "    if lemmatize:\n",
        "        output_lem = [nltk.pos_tag(w) for w in words]\n",
        "        return [[lemmatizer.lemmatize(x[0], pos = nltk2wn_tag(x[1])) for x in w] for w in output_lem]\n",
        "    else:\n",
        "        return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uat6ezTDuV9q",
        "outputId": "fc892487-35cd-4c1b-847f-05e886419376"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNBH2qaquYuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e4fd0f-9366-49b4-e7da-f2eb7c6e9cc9"
      },
      "source": [
        "start = time.time()\n",
        "trail1['tokenized_words'] = trail1.apply(lambda row: tokenize(row['plots'], stop, lemmatize = True), axis=1)\n",
        "end = time.time()\n",
        "print(\"Total Time to tokenize plots:\", end - start, \"seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Time to tokenize plots: 1122.765466928482 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtE8Z8Abul-P"
      },
      "source": [
        "trail1['flattened_tokens'] = trail1.apply(lambda l: [item for sublist in l['tokenized_words'] for item in sublist], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "dKQseXjQ9tvh",
        "outputId": "36f82dbe-7bbf-42a6-be13-9647a88e6b4a"
      },
      "source": [
        "trail1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plots</th>\n",
              "      <th>movie_name</th>\n",
              "      <th>genres</th>\n",
              "      <th>tokenized_words</th>\n",
              "      <th>flattened_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Shlykov, a hard-working taxi driver and Lyosha, a saxophonist, develop a bizarre love-hate relationship, and despite their prejudices, realize they aren't so different after all.</td>\n",
              "      <td>Taxi Blues</td>\n",
              "      <td>Drama</td>\n",
              "      <td>[[shlykov, hard, work, taxi, driver, lyosha, saxophonist, develop, bizarre, love, hate, relationship, despite, prejudice, realize, n't, different, .]]</td>\n",
              "      <td>[shlykov, hard, work, taxi, driver, lyosha, saxophonist, develop, bizarre, love, hate, relationship, despite, prejudice, realize, n't, different, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...</td>\n",
              "      <td>The Hunger Games</td>\n",
              "      <td>Science Fiction</td>\n",
              "      <td>[[nation, panem, consist, wealthy, capitol, twelve, poor, district, .], [punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, .], [tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, .], [first, reap, DG, year,...</td>\n",
              "      <td>[nation, panem, consist, wealthy, capitol, twelve, poor, district, ., punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, ., tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, ., first, reap, DG, year, old, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...</td>\n",
              "      <td>The Hunger Games</td>\n",
              "      <td>Action</td>\n",
              "      <td>[[nation, panem, consist, wealthy, capitol, twelve, poor, district, .], [punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, .], [tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, .], [first, reap, DG, year,...</td>\n",
              "      <td>[nation, panem, consist, wealthy, capitol, twelve, poor, district, ., punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, ., tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, ., first, reap, DG, year, old, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...</td>\n",
              "      <td>The Hunger Games</td>\n",
              "      <td>Drama</td>\n",
              "      <td>[[nation, panem, consist, wealthy, capitol, twelve, poor, district, .], [punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, .], [tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, .], [first, reap, DG, year,...</td>\n",
              "      <td>[nation, panem, consist, wealthy, capitol, twelve, poor, district, ., punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, ., tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, ., first, reap, DG, year, old, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Poovalli Induchoodan  is sentenced for six years prison life for murdering his classmate. Induchoodan, the only son of Justice Maranchery Karunakara Menon  was framed in the case by Manapally Madhavan Nambiar  and his crony DYSP Sankaranarayanan  to take revenge on idealist judge Menon who had e...</td>\n",
              "      <td>Narasimham</td>\n",
              "      <td>Action</td>\n",
              "      <td>[[poovalli, induchoodan, sentence, six, year, prison, life, murder, classmate, .], [induchoodan, son, justice, maranchery, karunakara, menon, frame, case, manapally, madhavan, nambiar, crony, dysp, sankaranarayanan, take, revenge, idealist, judge, menon, earlier, give, jail, sentence, manapally,...</td>\n",
              "      <td>[poovalli, induchoodan, sentence, six, year, prison, life, murder, classmate, ., induchoodan, son, justice, maranchery, karunakara, menon, frame, case, manapally, madhavan, nambiar, crony, dysp, sankaranarayanan, take, revenge, idealist, judge, menon, earlier, give, jail, sentence, manapally, co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                         plots  ...                                                                                                                                                                                                                                                                                             flattened_tokens\n",
              "0                                                                                                                           Shlykov, a hard-working taxi driver and Lyosha, a saxophonist, develop a bizarre love-hate relationship, and despite their prejudices, realize they aren't so different after all.  ...                                                                                                                                                         [shlykov, hard, work, taxi, driver, lyosha, saxophonist, develop, bizarre, love, hate, relationship, despite, prejudice, realize, n't, different, .]\n",
              "1  The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...  ...  [nation, panem, consist, wealthy, capitol, twelve, poor, district, ., punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, ., tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, ., first, reap, DG, year, old, p...\n",
              "2  The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...  ...  [nation, panem, consist, wealthy, capitol, twelve, poor, district, ., punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, ., tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, ., first, reap, DG, year, old, p...\n",
              "3  The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...  ...  [nation, panem, consist, wealthy, capitol, twelve, poor, district, ., punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, ., tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, ., first, reap, DG, year, old, p...\n",
              "4  Poovalli Induchoodan  is sentenced for six years prison life for murdering his classmate. Induchoodan, the only son of Justice Maranchery Karunakara Menon  was framed in the case by Manapally Madhavan Nambiar  and his crony DYSP Sankaranarayanan  to take revenge on idealist judge Menon who had e...  ...  [poovalli, induchoodan, sentence, six, year, prison, life, murder, classmate, ., induchoodan, son, justice, maranchery, karunakara, menon, frame, case, manapally, madhavan, nambiar, crony, dysp, sankaranarayanan, take, revenge, idealist, judge, menon, earlier, give, jail, sentence, manapally, co...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX5xQpu6_PAA",
        "outputId": "141905e9-96e1-4d7c-c8e0-44565db560c4"
      },
      "source": [
        "type(trail1[\"movie_name\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gOYmpbovd1t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "d1d242a4-37ce-4638-df1a-81b09a20b99b"
      },
      "source": [
        "#Binarize labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(trail1[\"genres\"])\n",
        "trail1[\"binarized_labels\"] = labels.tolist()\n",
        "\n",
        "y = mlb.transform(trail1[\"genres\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-16bd3cc6af9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Binarize labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrail1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrail1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"binarized_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0mclass_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mclass_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;31m# sort classes and reorder columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, y, class_mapping)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNp03cJKvd6B",
        "outputId": "76c0e26f-3dea-4e91-9172-1d587889c0dc"
      },
      "source": [
        "mlb.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Action', 'Adventure', 'Comedy', 'Crime Fiction', 'Drama',\n",
              "       'Family Film', 'Horror', 'Romance Film', 'Science Fiction',\n",
              "       'Thriller'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80YKYu9bvds5",
        "outputId": "a6dcbd7f-0c37-4b34-e8e2-635563f89658"
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36050, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54HX_EDI4QPn"
      },
      "source": [
        "#Pickle Data\n",
        "trail1.to_pickle(\"./trail1.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "AN6YOZPa8Zze",
        "outputId": "e864ee79-ce44-475e-a94a-271279afd1c0"
      },
      "source": [
        "trail1 = pd.read_pickle(\"./trail1.pkl\")\n",
        "trail1.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plots</th>\n",
              "      <th>movie_name</th>\n",
              "      <th>genres</th>\n",
              "      <th>tokenized_words</th>\n",
              "      <th>flattened_tokens</th>\n",
              "      <th>binarized_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Shlykov, a hard-working taxi driver and Lyosha, a saxophonist, develop a bizarre love-hate relationship, and despite their prejudices, realize they aren't so different after all.</td>\n",
              "      <td>Taxi Blues</td>\n",
              "      <td>[Drama]</td>\n",
              "      <td>[[shlykov, hard, work, taxi, driver, lyosha, saxophonist, develop, bizarre, love, hate, relationship, despite, prejudice, realize, n't, different, .]]</td>\n",
              "      <td>[shlykov, hard, work, taxi, driver, lyosha, saxophonist, develop, bizarre, love, hate, relationship, despite, prejudice, realize, n't, different, .]</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...</td>\n",
              "      <td>The Hunger Games</td>\n",
              "      <td>[Science Fiction, Action, Drama]</td>\n",
              "      <td>[[nation, panem, consist, wealthy, capitol, twelve, poor, district, .], [punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, .], [tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, .], [first, reap, DG, year,...</td>\n",
              "      <td>[nation, panem, consist, wealthy, capitol, twelve, poor, district, ., punishment, past, rebellion, district, must, provide, boy, girl, age, DG, DG, select, lottery, annual, hunger, game, ., tribute, must, fight, death, arena, sole, survivor, reward, fame, wealth, ., first, reap, DG, year, old, p...</td>\n",
              "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                         plots  ...                binarized_labels\n",
              "0                                                                                                                           Shlykov, a hard-working taxi driver and Lyosha, a saxophonist, develop a bizarre love-hate relationship, and despite their prejudices, realize they aren't so different after all.  ...  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
              "1  The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole...  ...  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8dAVJh4rlsm"
      },
      "source": [
        "#Token Analysis\n",
        "Iterate over tokenized words and create dictionaries that keep track of number of tokens, length of sentences, and sentences per plot summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxdAIYeU-L0E",
        "outputId": "ed0417a4-9e8d-4370-a53c-39a4b244115e"
      },
      "source": [
        "word_dict = {}\n",
        "sent_per_summary_dict = {}\n",
        "word_per_sent_dict = {}\n",
        "rows = len(trail1['tokenized_words'])\n",
        "print(rows)#number of plot summaries\n",
        "for i in range(len(trail1['tokenized_words'])):\n",
        "    length = len(trail1['tokenized_words'][i])\n",
        "    if length in sent_per_summary_dict:\n",
        "        sent_per_summary_dict[length] += 1\n",
        "    else:\n",
        "        sent_per_summary_dict[length] = 1\n",
        "    for j in range(length):\n",
        "        word_count = len(trail1['tokenized_words'][i][j])\n",
        "        if word_count in word_per_sent_dict:\n",
        "            word_per_sent_dict[word_count] += 1\n",
        "        else:\n",
        "            word_per_sent_dict[word_count] = 1\n",
        "        for word in trail1['tokenized_words'][i][j]:\n",
        "            if word in word_dict:\n",
        "                word_dict[word] += 1\n",
        "            else:\n",
        "                word_dict[word] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUiqlAcGTxNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c76fbb-0400-45ce-b6aa-f452f464cd0e"
      },
      "source": [
        "\n",
        "print(len(word_dict.keys())) #should be number of unique words\n",
        "print(sum(word_dict.values())) #should be total number of words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "131861\n",
            "7172308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyVP9Uq0h5mA",
        "outputId": "dbff9432-368e-4082-e040-fc09c6eea8ce"
      },
      "source": [
        "count = 0\n",
        "twoOrOne = 0\n",
        "for value in word_dict.values():\n",
        "    if value == 1:\n",
        "        count +=1\n",
        "    if value <3:\n",
        "        twoOrOne +=1\n",
        "print(len(word_dict.keys()) - count) # words that appear more than once\n",
        "print(len(word_dict.keys()) - twoOrOne) # words that appear more than twice"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "76241\n",
            "60642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASHeN0Vlh8ah",
        "outputId": "f83c38ee-48fd-43db-9d82-3c92b8972cf1"
      },
      "source": [
        "print(len(word_per_sent_dict.keys())) #should be number of unique sentence lengths\n",
        "print(sum(word_per_sent_dict.values())) #should be number of sentences in all plots\n",
        "print(max(word_per_sent_dict.keys())) #should be largest sentence length\n",
        "total = 0\n",
        "weight_sum = 0\n",
        "for key, value in word_per_sent_dict.items():\n",
        "    total += value\n",
        "    weight_sum += key*value\n",
        "print(weight_sum/total) #should be average sentence length\n",
        "#print(word_per_sent_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "111\n",
            "587257\n",
            "273\n",
            "12.213235431846705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qHubsDGiACU",
        "outputId": "7390c4f8-f383-4e03-c744-11be62706609"
      },
      "source": [
        "print(len(sent_per_summary_dict.keys())) #should be number of unique sentence lengths per summary\n",
        "print(max(sent_per_summary_dict.keys())) #should be highest amount of sentences per summary\n",
        "total = 0\n",
        "weight_sum = 0\n",
        "for key, value in sent_per_summary_dict.items():\n",
        "    total += value\n",
        "    weight_sum += key*value\n",
        "print(weight_sum/total) #should be average sentence count per summary\n",
        "#print(sent_per_summary_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152\n",
            "321\n",
            "16.290069348127602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV-CHgTCp-wc"
      },
      "source": [
        "#Load GloVe Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxOpW3gyp-RK",
        "outputId": "fb4cd201-c459-4b3b-9d98-cb9ccfcf5fa5"
      },
      "source": [
        "#Load GloVe Word Embeddings\n",
        "#compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings\n",
        "embeddings_index = {}\n",
        "GLOVE_DIR = '/content/drive/MyDrive/deep LEarning Project_/'\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPfdk1jNqDrA"
      },
      "source": [
        "#create average word vector. This will later be used in place of unknown words\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'), 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        pass\n",
        "n_vec = i + 1\n",
        "hidden_dim = len(line.split(' ')) - 1\n",
        "\n",
        "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
        "\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'), 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
        "\n",
        "average_vec = np.mean(vecs, axis=0)\n",
        "#print(average_vec)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}